{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import pytz\n",
        "\n",
        "# Define la zona horaria de Perú\n",
        "peru_timezone = pytz.timezone('America/Lima')\n",
        "\n",
        "# Define la fecha de inicio (2014-01-01)\n",
        "start_date = datetime(2014, 1, 1)\n",
        "\n",
        "# Número de días para recopilar datos (0 para recopilar solo para la fecha inicial)\n",
        "num_days = 40\n",
        "\n",
        "# Si num_days es mayor que 0, calcula la fecha de finalización\n",
        "if num_days > 0:\n",
        "    end_date = start_date + timedelta(days=num_days - 1)\n",
        "else:\n",
        "    end_date = start_date\n",
        "\n",
        "# Lista para almacenar los DataFrames de cada página\n",
        "dfs = []\n",
        "\n",
        "# Mientras la fecha de inicio sea anterior o igual a la fecha de finalización\n",
        "while start_date <= end_date:\n",
        "    # Formatea la fecha en el formato \"yyyy-mm-dd\"\n",
        "    formatted_date = start_date.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    # Construye la URL con la fecha actual\n",
        "    cnn_url = f\"https://elcomercio.pe/archivo/todas/{formatted_date}/\"\n",
        "\n",
        "    # Realiza la solicitud HTTP\n",
        "    html = requests.get(cnn_url)\n",
        "    bsobj = BeautifulSoup(html.content, 'lxml')\n",
        "\n",
        "    # Inicializa las listas para almacenar los datos\n",
        "    headlines = []\n",
        "    redaccion = []\n",
        "    dates = []\n",
        "    images = []\n",
        "    links = []\n",
        "    comments = []\n",
        "\n",
        "    # Tu código de extracción de datos aquí\n",
        "    for link in bsobj.findAll(\"h2\"):\n",
        "        headlines.append(link.text)\n",
        "\n",
        "    for news in bsobj.find_all('a', {'class': 'story-item__author block uppercase mt-10 font-thin text-xs text-gray-200'}):\n",
        "        author_name = news.text.strip()\n",
        "\n",
        "        # Reemplazar espacios en blanco con \"No hay\" si el nombre está vacío\n",
        "        if not author_name:\n",
        "            author_name = \"No hay\"\n",
        "\n",
        "        # Agregar el nombre a la lista\n",
        "        redaccion.append(author_name)\n",
        "\n",
        "    for news in bsobj.find_all('p', {'class': 'story-item__date font-thin ml-5 text-xs text-gray-300 md:mt-5 md:ml-0'}):\n",
        "        dates.append(news.text.strip())\n",
        "\n",
        "    for news in bsobj.find_all('img', {'class': 'story-item__img object-cover object-center w-full h-full'}):\n",
        "        img_url = news.get('src')\n",
        "        if img_url:\n",
        "            images.append(img_url)\n",
        "\n",
        "    for index, news in enumerate(bsobj.find_all('a', {'class': 'story-item__title block overflow-hidden primary-font line-h-xs mt-10'})):\n",
        "        news_link = news.get('href')\n",
        "        if news_link:\n",
        "            full_link = \"https://elcomercio.pe\" + news_link\n",
        "            links.append(full_link)\n",
        "            #print(full_link)  # Imprimir el enlace\n",
        "        else:\n",
        "            link_text = \"No tiene link\"\n",
        "            links.append(link_text)\n",
        "\n",
        "            # Imprimir el mensaje cuando no se encuentra un enlace\n",
        "            #print(f\"Iteración {index}: {link_text}\")\n",
        "\n",
        "    for link in links:\n",
        "        if link == \"No tiene link\":\n",
        "            comment_text = \"No hay comentario disponible\"\n",
        "        else:\n",
        "            news_html = requests.get(link)\n",
        "            news_bsobj = BeautifulSoup(news_html.content, 'lxml')\n",
        "\n",
        "            # Buscar el primer tipo de elemento que contiene el comentario\n",
        "            comment_element = news_bsobj.find('h2', {'class': 'sht__summary'})\n",
        "\n",
        "            # Si no se encontró el primer tipo de elemento, buscar el segundo tipo\n",
        "            if not comment_element:\n",
        "                comment_element = news_bsobj.find('h2', {'class': 'story-header__news-summary pr-20 pl-20 mb-20 secondary-font line-h-sm text-gray-300 text-xl font-normal'})\n",
        "\n",
        "            if comment_element:\n",
        "                comment_text = comment_element.text.strip()\n",
        "            else:\n",
        "                comment_text = \"No hay comentario disponible\"\n",
        "\n",
        "        comments.append(comment_text)\n",
        "        #print(comment_text)\n",
        "\n",
        "    print(len(headlines))\n",
        "    print(len(redaccion))\n",
        "    print(len(dates))\n",
        "    print(len(images))\n",
        "    print(len(links))\n",
        "    print(len(comments))\n",
        "\n",
        "    # Crear un DataFrame con los datos recopilados\n",
        "    df = pd.DataFrame({\n",
        "        'Headline': headlines,\n",
        "        'Redaccion': redaccion,\n",
        "        'Date': dates,\n",
        "        'Image URL': images,\n",
        "        'Link': links,\n",
        "        'Comentarios': comments\n",
        "    })\n",
        "\n",
        "    # Agregar el DataFrame a la lista de DataFrames\n",
        "    dfs.append(df)\n",
        "\n",
        "    # Incrementa la fecha de inicio para la siguiente iteración\n",
        "    start_date += timedelta(days=1)\n",
        "\n",
        "# Concatena todos los DataFrames en uno solo\n",
        "final_df = pd.concat(dfs, ignore_index=True)\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "# Imprime el DataFrame final\n",
        "print(final_df)\n",
        "\n",
        "# Guarda el DataFrame final en un archivo CSV\n",
        "final_df.to_csv('elcomercio_data3.csv', index=False)\n",
        "final_df.to_excel('elcomercio_data3.xlsx', index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "5895de-2HZFI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}